# ... this work is based on a collaborative effort between
#   .________.                                                         ,--.
#   |        |                                                     .  (  (
#   |,-.    /   HR Wallingford               EDF - LNHE           / \_ \_/ .--.
#   /   \  /    Howbery Park,                6, quai Watier       \   )   /_   )
#    ,.  `'     Wallingford, Oxfordshire     78401 Cedex           `-'_  __ `--
#   /  \   /    OX10 8BA, United Kingdom     Chatou, France        __/ \ \ `.
#  /    `-'|    www.hrwallingford.com        innovation.edf.com   |    )  )  )
# !________!                                                       `--'   `--
#
# _____                         ________________________________________________
# ____/ TELEMAC Configurations /_______________________________________________/
#
#  A configuration file is made of essentially two parts:
#   1.- a section named [Configurations],
#       which is a mandatory section listing the other active sections through
#       the mandatory key configs:
#   2.- other user-named sections,
#       each defining one way of compiling and running TELEMAC for a particular
#       subset of modules, or a particular operating system.
#  Here is an example:
#
# .....................................
[Configurations]
configs:  my-config her.config
# his_config
[my-config]
my_key: my_value
[her.config]
her_key: her_value
[his_config]
his_key: his_value
# .....................................
#
# /!\ Please note:
#
#   - Sections are delimited by [name] in square brackets.
#   - Keys are name: followed by a column punctuation and a value field, which
#     could be empty
#   - Symbol # at the start of a line is considered as a comment and is ignored
#     in the parsing
#   - Indenting sections and keys is critical (i.e. no indentation)
#   - Names <name> are replaced by known values, either system dependents (such
#     as <root>, <config>, etc.) script dependent (such as <libs>, <objs>, etc.)
#   - User-defined names [name] within value fields are replaced by the key
#     value of the same name, of by the environment key of the same name.
#
# _____                    ______________________________________________     __
# ____/ Mandatory Section /______________________________________________| 1 |_/
#
#  The following section is mandatory. The python script will look for it and
#     parse the key configs: (space-delimited) as a list of user-named sections
#     also defined in this file.
#  Whether this file contains many many user-named sections or just one, the
#     key configs: lists those that are being activated.
#  Additional filtering of this list of activated configuration has programmed
#     in most python scripts, through the use of the option " -c name ", where
#     the [name] is one specific active section in this file.
#
#  ~~> active configurations:
[Configurations]
configs:  pyd shared solve
#
#  ~~> ignored configurations:
#         inc.mpi gf_o mod
#
# /!\ TODO:
#     Create a script that is capable of switching between active
#     configurations, but also active version of the TELEMAC system.
#
# _____                      ____________________________________________     __
# ____/ User-Named Sections /____________________________________________| 2 |_/
#
#  The user-named sections can be as many as a user requires, some of which can
#     be activated through the previous [Configurations] section.
#  One name plays a special role though: [general]. The section ['general] is
#     not mandatory, but if it is here, it will defined the default values for
#     keys to avoid repetition in all subsequent user-named sections.
#     user-defined keys can either be in the ['general'] section (if relevant
#     to all configurations) or within one or more specific configurations.
#
#  Each section will have a number of ( key / value field ), some mandatory and
#     some not, depending on the python script reading the configuration file.
#     Several examples are provided below.
#
# /!\ Please note:
#   - Several of the examples below gradually introduce certain concepts and
#     key definitions. Please go through them all.
#
# _____                                _________________________________      __
# ____/ Default definitions [general] /_________________________________| 2a |_/
#
#  The example below shows one section that helps when dealing with multiple
#     user-named sections in the same configuration file. This section is not
#     mandatory.
#
[general]
#
#  Several keys are here defined:
#
#   % modules: lists those parts of the TELEMAC system to be included in the
#     processing (space-delimited), where:
#     + "system" means everything
#     + "-name" means that the module named "name" will be excluded from
#     + "update" means that only files created through the process that need
#       an update will be updated (this is the default).
#     + "clean" means that all files created through the process shall be wiped
#       out and re-created.
#
# /!\ Please note:
#   - The last two value names defined above for the key modules: ("update" and
#     "clean") are not useful and will eventually be removed. "update" is
#     somewhat close to the default settings anyway, and "clean" can be
#     triggered through the command line option --clean
#   @ example: the command line would be something like:
#     > compileTELEMAC.py --clean
#
modules:    system -mascaret
#
#   % sfx_..: defines the file extensions commonly used on the user operating
#     system, where:
#     + sfx_lib: are usually either ".lib" or ".so" or ".a", etc. is used for
#       library files, depending on user preferences and operating system,
#       other value are accepted
#     + sfx_zip: can only be either ".zip" or ".gztar" for archiving files
#       (such as the packaging of compiled binaries for on-line downloading)
#     + sfx_mod: is usually ".mod", other values are accepted
#     + sfx_obj: is usually either ".o" or ".obj" depending on the compiler,
#       other values are accepted
#     + sfx_exe: is usually ".exe" for windows systems or remains empty for
#       linux systems, other values are accepted
#
# /!\ Please note:
#   - Not all suffixes are mandatory (depending on the python script used)
#   - Not all suffixes have to be in the [general] section, as some might be
#     specific to one particular section (for instance, sfx_pyf: and sfx_pyd:
#     are only defined where appropriate)
#
sfx_mod:    .mod
sfx_obj:    .o
sfx_lib:    .a
sfx_exe:    .exe
sfx_zip:    .zip
#
#   % val_..: were introduced with the validateTELEMAC.py script, at a time of
#     hectic developments, and specifically for this script. Two keys are
#     mandatory at this stage:
#     + val_root: defines what directory structure should be scanned for
#       validation (validateTELEMAC.py will recursively scan for XML files),
#       where " <root>\examples " is the default directory structure.
#     + val_rank: defines a filter by ranks. Each validation test case is
#       associated with a rank (primary integer number) depending on how long
#       it takes to run. rank 2, 3 and 5, are usually short and validated
#       every nights, while higher ranks 7, 11, 13, etc. would be validate
#       every weekend.
#       val_rank: can take several value such as "all" for all ranks, or "<5"
#       for rank 2 and 3 or "21" for ranks 3 and 7 (3x7=21), or any other
#       specific combination of primary numbers.
#
# /!\ Please note:
#   - The rank can be reset in the command line using the -k option of the
#     validateTELEMAC.py script. As such, it may be removed eventually.
#   @ example: the command line would be something like:
#     > validateTELEMAC.py -k 210 --modules telemac2d.gretel.partel
#   - The name <root> will be automatically replaced by the script by the
#     appropriate value of the root install of the TELEMAC system. Use of
#     <root> avoid hard-coded path in the configuration file, which can then be
#     simply copied from one install / version of the system to another one.
#
val_root:   <root>\examples
val_rank:   all
# also possible val_rank:   <3 >7 6
#
# _____                         ________________________________________      __
# ____/ Doxygen documentations /________________________________________| 2b |_/
#
#  The example below shows one of the simplest configuration, which can be used
#     by the python script doxygenTELEMAC.py to generate the DOXYGEN
#     documentation of the entire source code.
#   @ example: the command line would be something like:
#     > doxygenTELEMAC.py -c doxydocs
#
[doxydocs]
#
#  Two keys are here defined:
#   - module: lists those parts of the TELEMAC system to be included in the
#     processing (space-delimited)
#   - cmd_doxygen: point to the command line (and executable) to be executed
#     by the python script.
#
# /!\ Please note:
#   - The script doxygenTELEMAC.py takes three keys at least: modules:,
#     cmd_doxygen:, and sfx_zip:. Since sfx_zip: is not defined in this
#     section, it will take its value from the [general] section. If the
#     [general] setion was to be removed, the following would have to be added
#     to this section [doxydocs]:
#     sfx_zip: .zip
#   - There are various keys defined as cmd..:, which are being introduced in
#     various configurations below. In all cases, these represent commands that
#     are executed (through the python) as is they were command line
#     instructions
#
modules:    system -mascaret -nestor
cmd_doxygen:    c:\opentelemac\bin\doxygen\bin\doxygen.exe
#
# _____                   ______________________________________________      __
# ____/ Shared libraries /______________________________________________| 2c |_/
#
#  The example below shows one configuration used to just compile the system as
#     shared libraries. No executable will be created, yet (by default) the
#     main programs (defined by cmdf files) will define what shared libraries
#     are created.
#
[shared]
#
#  A new key is here introduced:
#   - brief: is simply here for the user to associate this configuration
#     [shared] to some explanatory text that will be displayed on screen as the
#     configuration is scanned.
#   @ example: the command line below would print the brief:
#     > config.py -c shared
#
brief: This will create shared libraries, for possible use with the
    Fortran API. (no need for cmd_exe nor sfx_exe here)
#
#  Here only the module API of the TELEMAC system is selected, i.e., only
#     "api.so" will be be created (as well as the intermediate dependencies
#     such as the object files, and the sub-libraries special4api.so,
#     bief4api.so, parallel4api.so, etc.).
#  It is noted that the sfx_lib has also been reset here instead of taking the
#     value from the default [general] section.
#
modules:    api
sfx_lib:    .so
#
#  Two keys are here defined:
#   - cmd_obj: defines the actual command line to be executed every time a
#     compiled object is created from a single Fortran file. The names <mods>
#     <incs> and <f95name> are replaced by the python script appropriately,
#     where:
#     + <f95name> is replaced by the current Fortran file being compiled into
#       an object file
#     + <incs> is replaced by the list of necessary directories to fulfil the
#       INCLUDE statements, if any. In the configuration [shared] there are
#       none
#     + <mods> is replaced by the list of necessary directories to fulfil the
#       USE statements, or where the compiled module files are.
#   - cmd_lib: defines the actual command line to be executed every time a
#     group of object files are archived into a library. The names <libname>
#     <objs> and <libs> are replaced by the python script appropriately, where:
#     + <libname> is replaced by the name of the library file
#     + <objs> is replaced by the list of object files making up that
#       particular library
#     + <libs> is replaced by the list of necessary libraries also linked to
#       that particular library (dependencies).
#
# /!\ Please note:
#   - The user is free to modify the command lines to point to any compiler or
#     compiler options as required for that configuration.
#   - The following options could be relevant for gfortran:
#   @ example for cmd_obj:
#       -fbounds-check -ffpe-trap=zero,overflow,underflow,invalid -Wall
#       -finit-real=nan -fbacktrace -finit-real=nan
#   @ example for cmd_exe:
#       -lgcov --coverage -v
#
gfo.bin : C:\opentelemac\bin\gcc\bin
debug: [traps] [flags]
traps: -Wall
   -fbounds-check -ffpe-trap=zero,overflow,underflow
   -fbacktrace -finit-real=nan
cover: -lgcov --coverage -v
flags: -fconvert=big-endian -frecord-marker=4
cmd_obj:  [gfo.bin]\gfortran
   -fPIC -c -Og -cpp [debug] <mods> <incs> <f95name>
cmd_lib:  [gfo.bin]\gfortran
   -shared -fPIC -g [flags] -lm -lz -o <libname> <objs> <libs>
#
# /!\ Please note:
#   - The user is free to define keys, here above are the example of "flags",
#     "debug", "traps", "gfo.bin" and "cover" (the last one not being used),
#     and to insert these references anywhere else, in order to limit
#     repetitions within one's configuration files.
#   - Most keys support a multi-line assignment. Here above, the keys "cmd_obj"
#     and "cmd_lib" are defined over two lines, but will be executed as a one
#     line command by the python scripts. Another example is the user-defined
#     key "trap", which is defined over several 3 lines (note the indentation).
#     Its value, however, is not converted into a one line, until it is being
#     used by "cmd_obj" via the "debug" key.
#
# _____                              ___________________________________      __
# ____/ Standard serial compilation /___________________________________| 2d |_/
#
#  The example below shows one of the most commonly used configuration, to
#     compile the system (as executables) for use in serial mode on one
#     computer.
#
[gfor-one-dbg]
#
brief:     This will create standard executable, for serial mode.
#
#  The three keys below are here defined:
#   - cmd_obj: defines the actual command line to be executed every time a
#     compiled object is created from a single Fortran file (see above).
#   - cmd_lib: defines the actual command line to be executed every time a
#     group of object files are archived into a library (see above).
#   - cmd_exe: defines the actual command line to be executed every time an
#     executable is created from a number of objects and libraries. The names
#     <exename>, <objs> and <libs> are replaced by the python script
#     appropriately, where:
#     + <exename> is replaced by the name of the executable file
#     + <objs> is replaced by a subset of object files making up that
#       particular executable, in addition to which ...
#     + <libs> is replaced by the list of necessary libraries also linked to
#       that particular executable (dependencies).
#
cmd_obj:   gfortran -c -g -cpp  -fconvert=big-endian -frecord-marker=4 <mods> <incs> <f95name>
cmd_lib:   ar cru <libname> <objs>
cmd_exe:   gfortran -fconvert=big-endian -frecord-marker=4 -lgcov --coverage -v -lm -o <exename> <objs>  <libs>
#
#  The three new keys below are here defined:
#   - mods_all: provides the template to the addition of modules (.mod) to the
#     replacement of <mods> in the compilation commands above, and possibly a
#     link to the external modules.
#   - incs_all: provides the template to the list of directories for the
#     include files to the replacement of <incs> in the compilation commands
#     above, typically used to refer to external include file locations.
#   - libs_all: provides the template to the addition of libraries (.lib) to the
#     replacement of <libs> in the compilation commands above, and possibly a
#     link to the external libraries.
#  These keys are extendable by replacing "all" by a specific module, for
#     instance mods_parallel:, the template of which would then only apply to
#     the compilation of files with the parallel module.
#
mods_all:  -I<config>
incs_all:
libs_all:
#
#  For illustration purposes, here below how the same configuration would be
#     based on the Intel Fortran compiler.
# .....................................
[intel-one-opt]
cmd_obj:  ifort.exe /c /Ot /iface:cref /iface:nomixed_str_len_arg /nologo /fpp /names:uppercase /convert:big_endian <mods> <incs> <f95name>
cmd_lib:  xilib.exe /nologo /out:<libname> <objs>
cmd_exe:  xilink.exe /nologo /subsystem:console /stack:536870912 /out:<exename> <objs> <libs>
#
mods_all:   /include:<config>
sfx_obj:    .obj
#
#
# _____                         ________________________________________      __
# ____/ Another shared library /________________________________________| 2e |_/
#
#  The example below shows one configuration used to compile the one specific
#     Fortran file (and its dependencies) as one shared libraries. Again, no
#     executable will be created, but a new cmdf file will need to created for
#     a record of that tree dependency (using the command line option --rescan
#     of the compileTELEMAC.py script).
#
[another-shared]
#
#  A new key is here introduced:
#   - tag_..: which points to Fortran files that will be made top-of-the-tree
#     of the compilation, each Fortran file being associated to its own
#     library.
#  The name after the tag_ key ("handle") is user defined and used by the
#     compilation to highlight that tree structure amongst other tree
#     structures (usually driven by main programs). Eventually, the library
#     handle.so will be created (along with all the dependent object files, and
#     other sub-libraries such as special4handle.so, bief4handle.so, etc.)
#
tag_handle:  api_handle_var_t2d.f
#
#  Here the key modules: points to "handle", which is now defined as it own
#     top-of-the-tree structure thanks to the tag_..: key of the same name.
#
# /!\ Please note:
#   - The key modules: can also include other values, such as telemac2d, etc.
#   - A --rescan is necessary here to define the new top-of-the-tree and its
#     tree dependencies. As such, the file api_handle_var_t2d.cmdf will be
#     created for future compilation.
#
modules:     handle
sfx_lib:    .so
#
# /!\ Please note:
#   - This configuration is not very different to the [shared] configurtaion,
#     except for the new module name "handle", defined by the user through the
#     key tag_handle, highlighting a new top-of-the-tree, distinct from the
#     main programs with the TELEMAC system.
#
cmd_obj:    gfortran -fPIC -c -Og -cpp -fconvert=big-endian -frecord-marker=4 <mods> <incs> <f95name>
cmd_lib:    gfortran -shared -fPIC -g -fconvert=big-endian -frecord-marker=4 -lm -lz -o <libname> <objs> <libs>
#
# _____                 ________________________________________________      __
# ____/ Python Modules /________________________________________________| 2f |_/
#
#      This featured configuration has not been fully tested yet.
[pyd]
#
brief: This creates libraries but most importantly the pyd file, for use with
       the python API, for instance. Note the use of a tag key (to avoid
       refactoring of names) as opposed to an add key.
#
# Command for the intermediate pyf
# cmd_pyf:    f2py --quiet -h <pyfname> -m <pydname> <f95name>
#
# Command for the pyd
# cmd_pyd:    f2py --quiet -c <pyfname>
#     --build-dir <bdir>
#     --fcompiler=gnu95 --compiler=minqw32 <mods> <incs> <libs>
#
modules:    test
tag_test:  solve.f
#  api_handle_var_t2d.f api_handle_var_sis.f
#
cmd_obj:    gfortran -fPIC -c -Og -cpp -fconvert=big-endian -frecord-marker=4 <mods> <incs> <f95name>
cmd_lib:    gfortran -shared -fPIC -g -fconvert=big-endian -frecord-marker=4 -lm -lz -o <libname> <objs> <libs>
sfx_lib:    .a
cmd_pyd_test: f2py --build-dir .\test.solve\ -c <f95name> -m <pydname> --fcompiler=gnu95 --compiler=mingw32 <mods> <incs> <libs>
sfx_pyd:    .pyd
#
mods_all:  -I<config>
incs_all:
libs_all:
#
# _____                   ______________________________________________      __
# ____/ Dual Compilation /______________________________________________| 2g |_/
#
#  The example below shows one configuration used to compile TELEMAC-2D,
#     including a link to two different way of calling the SOLVE subroutine:
#   - The active mode remains unchanged, i.e. SOLVE remains and is compiled
#     together with the rest of TELEMAC-2D.
#   - The passive mode creates a completely different tree structure, starting
#     at the top with SOLVE, and compiling that entire tree with specific
#     (different) compilation options than the rest of TELEMAC-2D (as defined
#     by the key cmd_obj_dual:)
#
#  In order for the two compiled SOLVE objects (and dependents) to coexist, the
#     passive mode is entirely refactored as SOLVE_DUAL (and idem for
#     dependents) and compiled as a separate library, which get to be linked to
#     the final executable. The name DUAL is user defined and taken from the
#     keys tag_dual and add_dual.
#
[dual-solve]
#
brief: This creates separately both a "passive" mode (add_) and an "active"
       mode (tag_) of a subset of Fortran files and there dependencies. In
       order to coexist, the "passive" mode is refactored into .._DUAL, and
       linked up to the main executable as an external library.
       In this instance, you have to add a call to SOLVE_DUAL using compiler
       directive from within your main TELEMAC-2D code.
#
modules:      telemac2d
#
#  In addition to the key tag_..: another new key is here introduced:
#   - add_..: which also points to the same Fortran files, up for refactoring
#     and dual compilation.
#  The name after the add_ and tag_ keys ("solve") is user defined and used by
#     the compilation to highlight that tree structure amongst other tree
#     structures (usually driven by main programs). Eventually, the library
#     dual.solve.so will be created (along with all the dependent object
#     files, and other sub-libraries such as special4dual.solve.so,
#     bief4dual.solve.so, etc.)
#
tag_dual:    solve.f
add_dual:    solve.f
cmd_obj_dual:  gfortran -c -Og -cpp -fbounds-check -ffpe-trap=zero,overflow,underflow -Wall -fbacktrace -finit-real=nan -fconvert=little-endian -frecord-marker=4 -ffixed-line-length-132 <mods> <incs> <f95name>
#
cmd_obj:    gfortran -c -g -cpp -fconvert=little-endian -frecord-marker=4 <mods> <incs> <f95name>
cmd_lib:    ar cru <libname> <objs>
cmd_exe:    gfortran -fconvert=little-endian -frecord-marker=4 -v -lm -o <exename> <objs> <libs>
#
mods_all:  -I <config>
#
#
# _____                     ____________________________________________      __
# ____/ USE or INCLUDE mpi /____________________________________________| 2h |_/
#
#  The following two configurations are set to compile the TELEMAC system for
#    use in parallel on a desktop computer. The main difference between the two
#    configurations is that one includes header files from the MPICH2 library
#    (through the include 'mpif.h' statement), while the other use the MPI module
#    directly (through the USE MPI statement).
#
#  The USE version is best for consistent compilation, including checks on type
#    transfered between TELEMAC and MPI. However, it means that the MPI module
#    is also re-compiled with the same option and compiler as for the TELEMAC
#    system itself, which is not always possible.
#
# .....................................
[gfor-inc.mpi-dbg]
mpi_cmdexec:   mpiexec.exe -wdir <wdir> -n <ncsize> <exename>
#
cmd_obj:    gfortran -c -g -cpp -fconvert=big-endian -DHAVE_MPI -frecord-marker=4 <mods> <incs> <f95name>
cmd_lib:    ar cru <libname> <objs>
cmd_exe:    gfortran -fconvert=big-endian -frecord-marker=4 -v -lm -o <exename> <objs> <libs>
#
incs_all:  -I C:\opentelemac\bin\mpich2\include
mods_all:  -I <config>
libs_all:  C:\opentelemac\bin\mpich2\lib\libfmpich2g.a C:\opentelemac\lib\metis-5.0.2\lib\libmetis.a
# .....................................
[gfor-use.mpi-dbg]
mpi_cmdexec:   mpiexec.exe -wdir <wdir> -n <ncsize> <exename>
#
cmd_obj:    gfortran -c -g -cpp -fconvert=big-endian -DHAVE_MPI -DHAVE_MPI_MOD -frecord-marker=4 <mods> <incs> <f95name>
cmd_lib:    ar cru <libname> <objs>
cmd_exe:    gfortran -fconvert=big-endian -frecord-marker=4 -v -lm -o <exename> <objs> <libs>
#
incs_all:
mods_all:  -I <config> -I C:\opentelemac\bin\mpich2\include
libs_all:  C:\opentelemac\bin\mpich2\lib\libfmpich2g.a C:\opentelemac\lib\metis-5.0.2\lib\libmetis.a
# .....................................
#
#  The differences are as follows:
#   - cmd_obj: of [gfor-use.mpi-dbg] uses the  -DHAVE_MPI_MOD compiler directive.
#   - incs_all: of [gfor-use.mpi-dbg] is now empty
#   - mods_all: of [gfor-use.mpi-dbg] now include a directory to where the
#     mpi.mod file is.
#
#  Of course, your Fortran also has to account for both alternative, see in
#     PLANTE for instance:
#
#|    #if defined HAVE_MPI
#|    #  if defined HAVE_MPI_MOD
#|          USE MPI
#|          IMPLICIT NONE
#|    #  else
#|          IMPLICIT NONE
#|          INCLUDE 'mpif.h'
#|    #  endif
#|    #endif
#
#  For illustration purposes, here below how the include configuration would be
#     based on the Intel Fortran compiler, on a windows operating system.
# .....................................
[intel-inc.mpi-opt]
mpi_cmdexec:   C:\opentelemac\mpi\bin\mpiexec.exe -wdir <wdir> -n <ncsize> <exename>
#
cmd_obj:  ifort.exe /c /Og /QxHost /iface:cref /iface:nomixed_str_len_arg /nologo /fpp /names:uppercase /convert:big_endian /DHAVE_MPI <mods> <incs> <f95name>
cmd_lib:  xilib.exe /nologo /out:<libname> <objs>
cmd_exe:  xilink.exe /nologo /subsystem:console /stack:536870912 /nodefaultlib:libc.lib /out:<exename> <objs> <libs>
#
mods_all:   /include:<config>
incs_all:   /include:C:\opentelemac\mpi\include
libs_all:   C:\opentelemac\mpi\lib\fmpich2.lib C:\opentelemac\lib\libmetis.lib
#
sfx_obj:    .obj
# .....................................
#
#  For illustration purposes, here below how the include configuration would be
#     based on the Sun F95 compiler, on a linux operating system.
# .....................................
[sunf95-inc.mpi-opt]
mpi_cmdexec: /usr/bin/mpiexec -wdir <wdir> -n <ncsize> <exename>
#
cmd_obj:    sunf95 -c -fast -fsimple=1 -DHAVE_MPI -xlibmopt -xfilebyteorder=big8:%all -e -xildoff <mods> <incs> <f95name>
cmd_lib:    ar -cr <libname> <objs>
cmd_exe:    sunf95 -fast -fsimple=1 -xlibmopt -xfilebyteorder=big8:%all -e -xildoff -o <exename> <objs> <libs>
#
mods_all:   -M<config>
#
incs_all:  -I /usr/lib/openmpi/include/
libs_all:  /home/telemac/metis-5.0.2/build/Linux-x86_64/libmetis/libmetis.a  /usr/lib/openmpi/lib/libmpi.so
#
sfx_zip:    .gztar
sfx_lib:    .a
sfx_exe:
# .....................................
#
#
# _____               __________________________________________________      __
# ____/ HPC Clusters /__________________________________________________| 2i |_/
#
#  The following five configurations, highlight five possible use of TELEMAC on
#     a cluster, whether the simulation is made on the head node (testing), on
#     the compute nodes or a little of both, and whether in parallel or in
#     serial mode.
#
#  The following configurations are based on CentOS7, openmpi and the sbatch
#     queuing system. Other variations are provided further below.
#
# /!\ Please note:
#   - The indentation of the hpc_stdin: key is critical.
#
#  Here is the general: for these 5 configurations:
# .....................................
[general]
modules:    system -mascaret
#
cmd_lib:    ar cru <libname> <objs>
#
mods_all:   -I <config>
#
sfx_zip:    .gztar
sfx_lib:    .lib
sfx_obj:    .o
sfx_mod:    .mod
sfx_exe:
# .....................................
#
#  Cluster configuration 1:
#    Serial mode on the head node.
#    It will work whether ncsize (total number of cores) is 0 or 1
# .....................................
[hydro]
cmd_obj:    gfortran -c -O3 -fconvert=big-endian -frecord-marker=4 <mods> <incs> <f95name>
cmd_exe:    gfortran -fconvert=big-endian -frecord-marker=4 -v -lm -o <exename> <objs> <libs>
# .....................................
#
#  Cluster configuration 2:
#    Parallel mode on the head node, using mpiexec directly.
#    The only difference with [hydro] is the presence of the key mpi_cmdexec
#    and the -DHAVE_MPI compilation directive.
#    It will work whether ncsize (total number of cores) is 0 or more.
# .....................................
[hydru]
mpi_cmdexec: /apps/openmpi/2.1.0/gcc/6.3.0/bin/mpiexec -wdir <wdir> -n <ncsize> <exename>
#
cmd_obj:    gfortran -c -Og -cpp -DHAVE_MPI -DHAVE_MUMPS -fconvert=big-endian -frecord-marker=4 <mods> <incs> <f95name>
cmd_exe:    /apps/openmpi/2.1.0/gcc/6.3.0/bin/mpif90 -frecord-marker=4 -fconvert=big-endian -v -lm -lz -o <exename> <objs> <libs>
#
incs_all:  -I /apps/openmpi/2.1.0/gcc/6.3.0/include -I /home/HR/sbo/openlibs/MUMPS_5.0.0/include/
libs_all:  /apps/openmpi/2.1.0/gcc/6.3.0/lib/libmpi.so /home/HR/sbo/openlibs/metis-5.1.0/libmetis.a -L /home/HR/sbo/openlibs/MUMPS_5.0.0/lib -ldmumps -lmumps_common -lpord /home/HR/sbo/openlibs/SCALAPACK/libscalapack.a -L /home/HR/sbo/openlibs/BLAS -lblas /home/HR/sbo/openlibs/BLACS/LIB/blacs_MPI-LINUX-0.a /home/HR/sbo/openlibs/BLACS/LIB/blacsF77init_MPI-LINUX-0.a /home/HR/sbo/openlibs/BLACS/LIB/blacs_MPI-LINUX-0.a
# .....................................
#
#  Cluster configuration 3:
#    Parallel mode on the HPC queue, using mpiexec within the queue.
#    In that case, the file partitioning and assembly are done by the python on
#    the head node (the merge would have to be done manually after the run has
#    completed).
#    The only difference with [hydru] is the presence of the key hpc_cmdexec.
#    Of course, you also need the key hpc_stdin.
# .....................................
[hydra]
mpi_cmdexec: /apps/openmpi/2.1.0/gcc/6.3.0/bin/mpiexec -wdir <wdir> -n <ncsize> <exename>
#
cmd_obj:    gfortran -c -Og -cpp -DHAVE_MPI -DHAVE_MUMPS -fconvert=big-endian -frecord-marker=4 <mods> <incs> <f95name>
cmd_exe:    /apps/openmpi/2.1.0/gcc/6.3.0/bin/mpif90 -fconvert=big-endian -frecord-marker=4 -lpthread -v -lm -o <exename> <objs> <libs>
#
incs_all:  -I /apps/openmpi/2.1.0/gcc/6.3.0/include -I /home/HR/sbo/openlibs/MUMPS_5.0.0/include/
libs_all:  /apps/openmpi/2.1.0/gcc/6.3.0/lib/libmpi.so /home/HR/sbo/openlibs/metis-5.1.0/libmetis.a -L /home/HR/sbo/openlibs/MUMPS_5.0.0/lib -ldmumps -lmumps_common -lpord /home/HR/sbo/openlibs/SCALAPACK/libscalapack.a -L /home/HR/sbo/openlibs/BLAS -lblas /home/HR/sbo/openlibs/BLACS/LIB/blacs_MPI-LINUX-0.a /home/HR/sbo/openlibs/BLACS/LIB/blacsF77init_MPI-LINUX-0.a /home/HR/sbo/openlibs/BLACS/LIB/blacs_MPI-LINUX-0.a
#
hpc_stdin: #!/bin/bash
   #SBATCH -o <sortiefile>
   #SBATCH -e <exename>.err
   #SBATCH -J <jobname>
   #SBATCH -N <ncnode>
   #SBATCH --ntasks-per-node <nctile>
   #SBATCH -p <queue>
   <mpi_cmdexec>
   exit
hpc_cmdexec:  chmod 755 <hpc_stdin>; sbatch <hpc_stdin>
# .....................................
#
#  Cluster configuration 4:
#    Parallel mode on the HPC queue, using python script within the queue.
#    In that case, the file partitioning and assembly are done by the python
#    within the HPC queue. The only difference with [hydra] is the call to
#    <py_runcode> within the HPC_STDIN instead of <mpi_cmdexec>.
#    Note also that hpc_runcode replaces hpc_cmdexec
# .....................................
[hydry]
mpi_cmdexec: /apps/openmpi/2.1.0/gcc/6.3.0/bin/mpiexec -wdir <wdir> -n <ncsize> <exename>
#
cmd_obj:    gfortran -c -Og -cpp -DHAVE_MPI -DHAVE_MUMPS -fconvert=big-endian -frecord-marker=4 <mods> <incs> <f95name>
cmd_exe:    /apps/openmpi/2.1.0/gcc/6.3.0/bin/mpif90 -fconvert=big-endian -frecord-marker=4 -lpthread -v -lm -o <exename> <objs> <libs>
#
incs_all:  -I /apps/openmpi/2.1.0/gcc/6.3.0/include -I /home/HR/sbo/openlibs/MUMPS_5.0.0/include/
libs_all:  /apps/openmpi/2.1.0/gcc/6.3.0/lib/libmpi.so /home/HR/sbo/openlibs/metis-5.1.0/libmetis.a -L /home/HR/sbo/openlibs/MUMPS_5.0.0/lib -ldmumps -lmumps_common -lpord /home/HR/sbo/openlibs/SCALAPACK/libscalapack.a -L /home/HR/sbo/openlibs/BLAS -lblas /home/HR/sbo/openlibs/BLACS/LIB/blacs_MPI-LINUX-0.a /home/HR/sbo/openlibs/BLACS/LIB/blacsF77init_MPI-LINUX-0.a /home/HR/sbo/openlibs/BLACS/LIB/blacs_MPI-LINUX-0.a
#
hpc_stdin: #!/bin/bash
   #SBATCH -o <sortiefile>
   #SBATCH -e <exename>.err
   #SBATCH -J <jobname>
   #SBATCH -N <ncnode>
   #SBATCH --ntasks-per-node <nctile>
   #SBATCH -p <queue>
   source /etc/profile.d/modules.sh
   module load gcc/6.3.0 openmpi/2.1.0/gcc/6.3.0 canopy/1.7.4
   PATH=$PATH:$HOME/bin:~/opentelemac/main/scripts/python3
   export PATH
   cd <wdir>
   <py_runcode>
   exit
hpc_runcode:  chmod 755 <hpc_stdin>; sbatch <hpc_stdin>
# .....................................
#
#  Cluster configuration 5:
#    Serial mode on the HPC queue, using python script within the queue.
#    In that case, there is no partitioning nor assembly done by the python
#    within the HPC queue. The only differences with [hydry] is the compilation
#    without the directive HAVE_MPI and the absence of mpi_cmdexec.
#  Note also the presence of the hpc_depend, in case you wish to run more than
#    one simulation at a time.
# .....................................
[sedry]
cmd_obj:    gfortran -c -O3 -fconvert=big-endian -frecord-marker=4 <mods> <incs> <f95name>
cmd_exe:    gfortran -fconvert=big-endian -frecord-marker=4 -v -lm -o <exename> <objs> <libs>
#
hpc_stdin: #!/bin/bash
   #SBATCH -o <sortiefile>
   #SBATCH -e <exename>.err
   #SBATCH -J <jobname>
   #SBATCH -N <ncnode>
   #SBATCH --ntasks-per-node <nctile>
   #SBATCH -p <queue>
   source /etc/profile.d/modules.sh
   module load gcc/6.3.0 openmpi/2.1.0/gcc/6.3.0 canopy/1.7.4
   PATH=$PATH:$HOME/bin:~/opentelemac/main/scripts/python3
   export PATH
   cd <wdir>
   <py_runcode>
   exit
hpc_runcode:  chmod 755 <hpc_stdin>; sbatch <hpc_stdin>
hpc_depend: -W depend=afterok:<jobid>
# .....................................
#
#
# _____                  _______________________________________________      __
# ____/ HPC stdin files /_______________________________________________| 2j |_/
#
#  The following shows various example of HPC STDIN file, for the submission of
#     job on various queueing systems.
#
# .....................................< PBS >
#|   hpc_stdin: #!/bin/bash
#|      #PBS -S /bin/sh
#|      #PBS -o <sortiefile>
#|      #PBS -e <exename>.err
#|      #PBS -N <jobname>
#|      #PBS -l nodes=<nctile>:ppn=<ncnodes>
#|      #PBS -q <queue>
#|      <mpi_cmdexec>
#|      exit
#|   hpc_cmdexec:   chmod 755 <hpc_stdin>; qsub <hpc_stdin>
#
# .....................................< BSUB >
#|   hpc_stdin: #!/bin/bash
#|      #BSUB -n <ncsize>
#|      #BSUB -J <jobname>
#|      #BSUB -o <sortiefile>
#|      #BSUB -e <exename>.%J.err
#|      #BSUB -R "span[ptile=<nctile>]"
#|      <mpi_cmdexec>
#|      exit
#|   hpc_cmdexec:   chmod 755 <hpc_stdin>; bsub -q encore < <hpc_stdin>
#
# .....................................< QSUB >
#|   hpc_stdin: #!/bin/bash
#|      #$ -cwd                          # working directory is current directory
#|      #$ -V                            # forward your current environment to the execution environment
#|      #$ -pe mpi-<ncnodes>x1 <ncsize>  # no of cores requested
#|      #$ -S /bin/bash                  # shell it will be executed in
#|      #$ -j y                          # merge stderr and stdout
#|      cat $PE_HOSTFILE | awk '{print $1, " slots=<nctile>"}' > machinefile.$JOB_ID
#|      cat machinefile.$JOB_ID
#|      <mpi_cmdexec>
#|      exit
#|   hpc_cmdexec:   chmod 755 <hpc_stdin>; qsub < <hpc_stdin>
#
#
# _____                         ________________________________________________
# ____/ TELEMAC Configurations /_______________________________________________/
#
