# _____                              _______________________________
# ____/ TELEMAC Project Definitions /______________________________/
#
[Configurations]
#
configs:    hydru
#
# _____          ___________________________________________________
# ____/ GENERAL /__________________________________________________/
[general]
#
modules:    system -mascaret
#
cmd_lib:    ar cru <libname> <objs>
#
mods_all:   -I <config>
#
sfx_zip:    .gztar
sfx_lib:    .lib
sfx_obj:    .o
sfx_mod:    .mod
sfx_exe:
#
# _____                               ______________________________
# ____/ HYDRO -- Main EXE, Main node /_____________________________/
[hydro]
#
brief: scalar mode on the main node.
   TELEMAC will work whether processor is 0 or 1
#
cmd_obj:    gfortran -c -O3 -fconvert=big-endian -frecord-marker=4 <mods> <incs> <f95name>
cmd_exe:    gfortran -fconvert=big-endian -frecord-marker=4 -v -lm -o <exename> <objs> <libs>
#
# _____                              _______________________________
# ____/ HYDRU -- MPIEXEC, Main node /______________________________/
[hydru]
#
brief: parallel mode on the main node, using mpiexec directly.
       The only difference with hydro is the presence of the key
       mpi_cmdexec and the -DHAVE_MPI compilation directive.
       Of course, you also need the key par_cmdexec.
       Finally, note that this configuration also works whether
       processor is 0 or 1.
#
mpi_cmdexec: /apps/openmpi/2.1.0/gcc/6.3.0/bin/mpiexec -wdir <wdir> -n <ncsize> <exename>
#
cmd_obj:    gfortran -c -Og -cpp -fbounds-check -ffpe-trap=zero,overflow,underflow,invalid -Wall -finit-real=nan -fbacktrace -finit-real=nan -DHAVE_MPI -DHAVE_MUMPS -fconvert=big-endian -frecord-marker=4 <mods> <incs> <f95name>
cmd_exe:    /apps/openmpi/2.1.0/gcc/6.3.0/bin/mpif90 -frecord-marker=4 -fconvert=big-endian -v -lm -lz -o <exename> <objs> <libs>
#
incs_all:  -I /apps/openmpi/2.1.0/gcc/6.3.0/include/ -I /home/HR/sbo/openlibs/MUMPS_5.0.0/include/
libs_all:  /apps/openmpi/2.1.0/gcc/6.3.0/lib/libmpi.so  -L /home/HR/sbo/openlibs/MUMPS_5.0.0/lib -ldmumps -lmumps_common -lpord /home/HR/sbo/openlibs/SCALAPACK/libscalapack.a -L /home/HR/sbo/openlibs/BLAS /home/HR/sbo/openlibs/BLAS/blas_LINUX.a /home/HR/sbo/openlibs/BLACS/LIB/blacs_MPI-LINUX-0.a /home/HR/sbo/openlibs/BLACS/LIB/blacsF77init_MPI-LINUX-0.a /home/HR/sbo/openlibs/BLACS/LIB/blacs_MPI-LINUX-0.a /home/HR/sbo/openlibs/metis-5.1.0/libmetis.a
#
# _____                              _______________________________
# ____/ HYDRA -- MPIEXEC, HPC queue /______________________________/
[hydra]
#
brief: parallel mode on the HPC queue, using mpiexec within the queue.
   In that case, the file partitioning and assembly are done by the
   python on the main node.
   (the merge would have to be done manually)
   The only difference with hydru is the presence of the key
   hpc_cmdexec. Of course, you also need the key hpc_stdin.
#
mpi_hosts:    mgmt01
mpi_cmdexec: /apps/openmpi/1.10.2/gcc/6.1.0/bin/mpiexec -wdir <wdir> -n <ncsize> <exename>
#
par_cmdexec:   <config>/partel  < PARTEL.PAR >> <partel.log>
#
hpc_stdin: #!/bin/bash
   #PBS -S /bin/sh
   #PBS -o <sortiefile>
   #PBS -e <exename>.err
   #PBS -N <jobname>
   #PBS -l nodes=<nctile>:ppn=<ncnode>
   #PBS -q highp
   source /etc/profile.d/modules.sh
   module load gcc/6.1.0 openmpi/1.10.2/gcc/6.1.0
   <mpi_cmdexec>
   exit
#
hpc_cmdexec:  chmod 755 <hpc_stdin>; qsub <hpc_stdin>
#
cmd_obj:    gfortran -c -O3 -fconvert=big-endian -DHAVE_MPI -frecord-marker=4 <mods> <incs> <f95name>
cmd_exe:    /apps/openmpi/1.10.2/gcc/6.1.0/bin/mpif90 -fconvert=big-endian -frecord-marker=4 -lpthread -v -lm -o <exename> <objs> <libs>
#
# _____                             ________________________________
# ____/ HYDRY -- Python, HPC queue /_______________________________/
[hydry]
#
brief: parallel mode on the HPC queue, using python script within
   the queue.
   In that case, the file partitioning and assembly are done by
   the python within the HPC queue.
   The only difference with hydra is the call to <py_runcode> within
   the HPC_STDIN instead of <mpi_cmdexec>.
   Note also that hpc_runcode replaces hpc_cmdexec
#
mpi_hosts:    mgmt01
mpi_cmdexec: /apps/openmpi/1.10.2/gcc/6.1.0/bin/mpiexec -wdir <wdir> -n <ncsize> <exename>
#
par_cmdexec:   <config>/partel  < PARTEL.PAR >> <partel.log>
#
hpc_stdin: #!/bin/bash
   #PBS -S /bin/sh
   #PBS -o <sortiefile>
   #PBS -e <exename>.err
   #PBS -N <jobname>
   #PBS -l nodes=<nctile>:ppn=<ncnode>
   #PBS -q highp
   source /etc/profile.d/modules.sh
   module load gcc/6.1.0 openmpi/1.10.2/gcc/6.1.0 canopy/1.7.4
   PATH=$PATH:$HOME/bin:~/opentelemac/trunk/scripts/python27
   export PATH
   cd <wdir>
   <py_runcode>
   exit
#
hpc_runcode:   chmod 755 <hpc_stdin>; qsub <hpc_stdin>
#
hpc_depend: -W depend=afterok:<jobid>
#
cmd_obj:    gfortran -c -O3 -fconvert=big-endian -DHAVE_MPI -frecord-marker=4 <mods> <incs> <f95name>
cmd_exe:    /apps/openmpi/1.10.2/gcc/6.1.0/bin/mpif90 -fconvert=big-endian -frecord-marker=4 -lpthread -v -lm -o <exename> <objs> <libs>
#
# _____                             ________________________________
# ____/ SEDRY -- Python, HPC queue /_______________________________/
[sedry]
#
brief: serial mode on the HPC queue, using python script within
   the queue.
   In that case, there is no partitioning nor assembly done by
   the python within the HPC queue.
   The only differences with hydry is the compilation without the
   directive HAVE_MPI and the absence of mpi_hosts, mpi_cmdexec and
   the par_cmdexec.
#
hpc_stdin: #!/bin/bash
   #PBS -S /bin/sh
   #PBS -o <sortiefile>
   #PBS -e <exename>.err
   #PBS -N <jobname>
   #PBS -l nodes=<nctile>:ppn=<ncnode>
   #PBS -q highp
   source /etc/profile.d/modules.sh
   module load gcc/6.1.0 openmpi/1.10.2/gcc/6.1.0 canopy/1.7.4
   PATH=$PATH:$HOME/bin:~/workspace/label/hydra/fishtank/scripts/python27
   export PATH
   cd <wdir>
   <py_runcode>
   exit
#
hpc_depend: -W depend=afterok:<jobid>
#
hpc_runcode:   chmod 755 <hpc_stdin>; qsub <hpc_stdin>
#
cmd_obj:    gfortran -c -O3 -fconvert=big-endian -frecord-marker=4 <mods> <incs> <f95name>
#
#
